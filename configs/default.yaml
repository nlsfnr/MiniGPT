batch_size: 128
use_half_precision: true
loss_scale_period: 50
initial_loss_scale_log2: 15
gradient_accumulation_steps: 4
peak_learning_rate: 0.002
end_learning_rate: 0.0001
warmup_steps: 100
total_steps: 10000
weight_decay: 0.01

# Model config
vocab_size: 30000
max_sequence_length: 32
num_layers: 4
num_heads: 8
key_size: 32
value_size: 32
w_init_var: 0.02
embed_init_var: 0.02
mlp_size: 256
model_size: 64
dropout: 0.1

# Data config
dataset_path: ./data/wikitext/
tokenizer_path: ./tokenizers/wikitext-30k

# DataLoader config
num_workers: 6
