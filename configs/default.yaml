batch_size: 256
use_half_precision: true
loss_scale_period: 500
initial_loss_scale_log2: 15
peak_learning_rate: 0.002
end_learning_rate: 0.0001
warmup_steps: 100
total_steps: 10000
weight_decay: 0.01
yield_freq: 10

# Model config
vocab_size: 4096
embedding_size: 64
max_sequence_length: 32
num_layers: 8
num_heads: 6
key_size: 32
value_size: 32
w_init_var: 0.02
embed_init_var: 0.02
mlp_size: 512
model_size: 256
dropout: 0.1

# Data config
datasets: ['imdb', 'wikitext', 'c4']
dataset_weights: [1, 10, 20]
min_frequency: 100
min_length: 32
shuffle_buffer_size: 500
tokenizer_path: ./tokenizers/combined-4k
tokenizer_kind: 'sentencepiece'

# DataLoader config
num_workers: 6
