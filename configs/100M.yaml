dataset:
  - args: [c4, en]  # 305 GB
    kwargs: {split: train}
    weight: 305  # 1:1
  - args: [wikitext, wikitext-103-raw-v1]  # 0.5 GB
    kwargs: {split: train}
    weight: 3  # 6:1
  - args: [bookcorpusopen]  # 6.3 GB
    kwargs: {split: train}
    weight: 37.8  # 6:1
  - args: [multi_news]  # 0.7 GB
    kwargs: {split: train, key: document}
    weight: 4.2  # 6:1

tokenizer:
  args: [bert-base-uncased]
  kwargs: {}

data:
  length: 512
  batch_size: 64
  shuffle_buffer_size: 25_000
  per_dataset_shuffle_buffer_size: 1_000

model:
  num_layers: 12
  vocabulary_size: 32_768
  embedding_dim: 384
  model_dim: 768
  num_heads: 12
  pos_emb_portion: 0.5
  hidden_dim: 2_304
  dropout: 0.0

# On an A100, we expect the step / second per device-batch to be around 0.2342.
# Then, in 24h, we have about 370_000 device batches.

optimizer:
  #                                      70_000       140_000
  gradient_accumulation_steps: [[0, 8], [8_750, 16], [13_125, 32]]
  weight_decay: 0.01
  lr_min: 0.00006
  lr_max: 0.0006
  lr_decay_steps: 20_312
  lr_warmup_steps: 1_000
  gradient_clip_norm: 1.0
  adam_b1: 0.9
  adam_b2: 0.999
  adam_eps: 0.00000001

mixed_precision:
  enable: true
  initial_scale_log2: 8
  scale_period: 5000
