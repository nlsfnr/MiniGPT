dataset:
  args: [c4, en]
  kwargs: {split: train}

tokenizer:
  args: [bert-base-uncased]
  kwargs: {}

data:
  length: 1024
  min_length: 768
  pad_token_id: 0
  batch_size: 32
  shuffle_buffer_size: 100_000

model:
  num_layers: 18
  vocabulary_size: 32_768
  embedding_dim: 512
  model_dim: 1152
  num_heads: 18
  pos_emb_portion: 0.5
  hidden_dim: 4_608
  dropout: 0.0

optimizer:
  gradient_accumulation_steps: [[0, 1], [10_000, 8], [25_000, 64]]
  weight_decay: 0.01
  lr_min: 0.00006
  lr_max: 0.0006
  lr_decay_steps: 500_000
  lr_warmup_steps: 1_000
  gradient_clip_norm: 1.0
  adam_b1: 0.9
  adam_b2: 0.999

mixed_precision:
  enable: true
  initial_scale_log2: 1
  scale_period: 1000
