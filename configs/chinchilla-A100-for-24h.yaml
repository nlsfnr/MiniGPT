batch_size: 64
use_half_precision: true
loss_scale_period: 2000
initial_loss_scale_log2: 15
peak_learning_rate: 0.0006
end_learning_rate: 0.00006
max_gradient_norm: 0.5
warmup_steps: 2000
total_steps: 100000
weight_decay: 0.01
yield_freq: 10

# Model config
vocab_size: 32768
embedding_size: 1152
max_sequence_length: 256
num_layers: 24
num_heads: 18
key_size: 64
value_size: 64
w_init_var: 0.02
embed_init_var: 0.02
mlp_size: 4608
model_size: 1152
dropout: 0.0
use_rotary_embedding: true

# Data config
datasets: ['wikitext', 'c4']
dataset_weights: [1, 20]
min_frequency: 100
min_length: 192
shuffle_buffer_size: 5000
tokenizer_path: ./tokenizers/wikitext-32k
tokenizer_kind: 'sentencepiece'

# DataLoader config
num_workers: 12
